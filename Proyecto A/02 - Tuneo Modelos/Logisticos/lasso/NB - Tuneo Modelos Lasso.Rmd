---
title: "NB - Tuneo Modelos"
output: html_document
---

https://topepo.github.io/caret/index.html
https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret

# Paquetes

```{r}
library(caret)
library(readr)
library(dplyr)
library(ggplot2)
```


```{r}
# Linux
data <- read_csv('Proyecto A/00 - Creación Dataset y Variables/Dataset_Modelos.csv')
```

```{r}
# Windows
data <- read_csv('../00 - Creación Dataset y Variables/Dataset_Modelos.csv')
```

Borramos las variables de caracteres.

```{r}
char_vars <- c("X1", 
               "url", 
               "scheme", 
               "domain_complete", 
               "domain", 
               "subdomain", 
               "suffix", 
               "domain_subdomain", 
               "path", 
               "suffix2")

df <- data %>%
  select(-char_vars)
```



```{r}
names(df)
```

Partimos el dataset en partición de Train y Test. El objeto *train_index* tendrá el índice de el Train set.

```{r}
set.seed(45)
train_index <- createDataPartition(y = df$phishing, times = 1, p = 0.80, list = FALSE)

train <- df[train_index,]
test <- df[-train_index,]

train <- train %>%
  mutate(phishing = factor(phishing, labels = c("NoPhish", "Phish")))
test <- test %>%
  mutate(phishing = factor(phishing, labels = c("NoPhish", "Phish")))

#test1 <- df[-train_index,]

#test_index <- createDataPartition(y = test1$phishing, times = 1, p = 0.5, list = FALSE)

#test <- test1[test_index,]
#validation <- test1[-test_index,]
```

Armamos los folds de Cross-Validation, usamos k = 5. El objeto *resample_method* tendrá toda la información de las particiones de Cross-Validation que realizamos.

```{r}
set.seed(53)
k = 5
CV_folds <- createFolds(train$phishing, k = k, list = TRUE)

resample_method <- trainControl(method = "cv",
                                number = k,
                                classProbs = TRUE,
                                summaryFunction = twoClassSummary,
                                verboseIter = TRUE,
                                returnData = TRUE,
                                returnResamp = "final",
                                savePredictions = "final",
                                search = "grid",
                                index = CV_folds)

```

En esta parte armamos el dataset con los valores de los hiperparámetros que vamos a probar. Importante: Las columnas tienen que llevar el nombre de los hiperparámetros de ese modelo en el método (paquete) empleado.

```{r}
lasso_grid <- expand.grid(alpha = 1,
                          lambda = seq(0.01, 1, 0.01))


dim(lasso_grid)
head(lasso_grid)
```

Corriendo esta celda se realiza Cross-Validation probando todas las combinaciones de hiperparámetros que determinamos previamente y se entrena el modelo final con todos los datos del Train set y con el set de hiperparámetros óptimos.

```{r}
lasso <- train(as.factor(phishing)~., 
               data = train, 
               method = "glmnet", 
               trControl = resample_method,
               verbose = TRUE,
               tuneGrid = lasso_grid,
               #tuneLength = 50,
               preProcess = c("center", "scale"),
               metric = "ROC")
```

Imprimimos los resultados obtenidos por Cross-Validation.
```{r}
plot(lasso, metric= "ROC")
plot(lasso, metric= "Spec")#ES SENSITIVITY
plot(lasso, metric= "Sens")#ES Specificity
```

```{r}
lasso
```

```{r}
lasso$bestTune
```

```{r}
lasso$preProcess
```
```{r}
lasso$results
```



```{r}
plot(lasso$finalModel, xvar = "lambda")
```

```{r}
ggplot(lasso$results, aes(x=lambda, y = Sens))+
  geom_point()+
  geom_line()+
  expand_limits(y = c(0,1))+
  theme_bw()
```


```{r}
dim(test)
```


Matriz de Confusión del modelo entrenado con el set de Test.

```{r}
confusionMatrix(data = as.factor(predict(lasso, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

# Guardar Resultados

En esta parte guardamos los resultados de la optimización de hiperparámetros mediante Cross-Validation. La celda comentada sirve para crear el archivo csv por primera vez. Una vez creado dejarla comentada y correr solamente la última celda para incluir los nuevos resultados.

**REEMPLAZAR EL NOMBRE DEL MODELO**
```{r}
getwd()
```

```{r}
#write_csv(lasso$results, file = 'lasso roc.csv')
```
```{r}
df_results<-lasso$results
write_csv(df_results, file = "lasso roc.csv",append = T)
```


```{r}
df_results <- read_csv("lasso roc.csv")
df_results <- rbind(df_results, lasso$results)
write_csv(df_results, file = "lasso roc.csv")
```
```{r}
df_results<-lasso$results
write_csv(df_results, file = "lasso roc.csv",append = T)
```

# Graficos del training
```{r}
ridge_results <- read_csv('ridge roc.csv')
lasso_results <- read_csv('lasso roc.csv')
resultados<-rbind(ridge_results[,0:8],lasso_results[,0:8])
#plot2<-rf_resultados %>%
  #filter(num.trees == 150
         #,max.depth!=0
         #,ROC>0.933
         #,min.node.size==c(30)
         #,mtry %in% c(1,2,5,150)
         #,mtry==c(5,70,150)
         #)
general_results<-append(lasso_results,ridge_results)
ggplot(resultados, aes(x=lambda, y = ROC,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,1))+
  theme_bw()
ggplot(resultados, aes(x=lambda, y = Sens,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,1))+
  theme_bw()
ggplot(resultados, aes(x=lambda, y = Spec,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,1))+
  theme_bw()
ggplot(resultados, aes(x=lambda, y = ROC,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,0.2))+
  theme_bw()
ggplot(resultados, aes(x=lambda, y = Sens,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,0.2))+
  theme_bw()
ggplot(resultados, aes(x=lambda, y = Spec,
                          color=as.factor(alpha)
                          ))+
  geom_point(size=3,position=position_jitter(h=0, w=0.02))+
  #geom_smooth()+
  #geom_line()+
  expand_limits(x=c(0,0.2))+
  theme_bw()
```

