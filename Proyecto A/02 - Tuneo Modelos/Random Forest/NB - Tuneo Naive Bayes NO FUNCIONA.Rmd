---
title: "NB - Tuneo Modelos"
output: html_document
---

https://topepo.github.io/caret/index.html
https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret

# Paquetes

```{r}
rm(list=ls())
library(caret)
library(readr)
library(dplyr)
library(ggplot2)
#library(ranger)

setwd(getwd())
```


```{r}
# Linux
data <- read_csv('Proyecto A/00 - Creación Dataset y Variables/Dataset_Modelos.csv')
```

```{r}
# Windows
setwd("~/GitHub/Proyecto_CDD")
getwd()
data <- read_csv("./Proyecto A/00 - Creación Dataset y Variables/Dataset_Modelos.csv")
```

Borramos las variables de caracteres.

```{r}
char_vars <- c(#"X1", 
               "...1",
               "url", 
               "scheme", 
               "domain_complete", 
               "domain", 
               "subdomain", 
               "suffix", 
               "domain_subdomain", 
               "path", 
               "suffix2")

df <- data %>%
  select(-char_vars)
```


```{r}
names(data)
```

Partimos el dataset en partición de Train y Test. El objeto *train_index* tendrá el índice de el Train set.

```{r}
set.seed(45)
train_index <- createDataPartition(y = df$phishing, times = 1, p = 0.80, list = FALSE)

train <- df[train_index,]
test <- df[-train_index,]

train <- train %>%
  mutate(phishing = factor(phishing, labels = c("NoPhish", "Phish")))
test <- test %>%
  mutate(phishing = factor(phishing, labels = c("NoPhish", "Phish")))
#test1 <- df[-train_index,]

#test_index <- createDataPartition(y = test1$phishing, times = 1, p = 0.5, list = FALSE)

#test <- test1[test_index,]
#validation <- test1[-test_index,]
```

```{r}
train$phishing
df$phishing[train_index]
```

Armamos los folds de Cross-Validation, usamos k = 5. El objeto *resample_method* tendrá toda la información de las particiones de Cross-Validation que realizamos.

```{r}
set.seed(53)
k = 5
CV_folds <- createFolds(train$phishing, k = k, list = TRUE)

resample_method <- trainControl(method = "cv",
                                number = k,
                                verboseIter = TRUE,
                                classProbs = TRUE,
                                returnData = TRUE,
                                summaryFunction = twoClassSummary,
                                returnResamp = "final",
                                savePredictions = "final",
                                #search = "random",
                                index = CV_folds)

```

En esta parte armamos el dataset con los valores de los hiperparámetros que vamos a probar. Importante: Las columnas tienen que llevar el nombre de los hiperparámetros de ese modelo en el método (paquete) empleado.

```{r}
ridge_grid <- expand.grid(fL=1:5,
                          usekernel=c(TRUE),
                          adjust = seq(0, 5, by = 1)
                          )
                            

dim(ridge_grid)
head(ridge_grid)
```

Corriendo esta celda se realiza Cross-Validation probando todas las combinaciones de hiperparámetros que determinamos previamente y se entrena el modelo final con todos los datos del Train set y con el set de hiperparámetros óptimos.

```{r}
library(klaR)
a<- Sys.time()

ridge <- train(as.factor(phishing)~., 
               data = train, 
               method = "nb", 
               trControl = resample_method,
               #verbose = TRUE,
               tuneGrid = ridge_grid,
               #preProcess = c("center", "scale"),
               #tuneLength = 25,

               metric = "ROC")


aux<-ridge$results
write_csv(aux,"Resultados_NB.csv")
#aux["num.tres"]=150
#aux["max.depth"]=depth
#write_csv(aux, file ="Resultados_RF.csv",append=T)
b<- Sys.time()
b-a

```

```{r}
rf_resultados <- read_csv("Resultados_RF.csv")

```

Imprimimos los resultados obtenidos por Cross-Validation.


```{r}
ridge
```

```{r}
ridge$bestTune
```

```{r}
ridge$preProcess
```



```{r} 
plot(ridge$results, xvar = "min.node.size")
plot(df_results, xvar = "min.node.size")
```



```{r}
dim(test)
```


Matriz de Confusión del modelo entrenado con el set de Test.

```{r}
confusionMatrix(data = as.factor(predict(ridge, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
ridge$finalModel
```

# Guardar Resultados

En esta parte guardamos los resultados de la optimización de hiperparámetros mediante Cross-Validation. La celda comentada sirve para crear el archivo csv por primera vez. Una vez creado dejarla comentada y correr solamente la última celda para incluir los nuevos resultados.

**REEMPLAZAR EL NOMBRE DEL MODELO**
```{r}

```

```{r}
getwd()

```

```{r}
getwd()
rf_resultados <- read_csv("Resultados_RF.csv")

write_csv(rf_resultados,"Resultados_RF.csv")
```

```{r}
rm(list=ls())
library(caret)
library(readr)
library(dplyr)
library(ggplot2)
library(ranger)

rf_resultados <- read_csv("Resultados_RF.csv")
aux<-rf_resultados %>%
  filter(num.trees==150,
         max.depth!=0)
sum(unique(aux[c("min.node.size")]))
```

```{r}
plot2<-aux %>%
  filter(
         #,ROC>0.933
         #min.node.size %in% c(1,30)
         #,mtry %in% c(1,2,5,50,150)
         mtry %in% c(1,2,5,20,70,150)
         )
ggplot(plot2, aes(x=max.depth, y = ROC,color=as.factor(mtry)))+
  geom_point(size=3,position=position_jitter(h=0, w=1))+
  geom_smooth(se=F,
              span=0.7
              )+
  scale_x_continuous(breaks = (seq(10,130, by = 20)))+
  #geom_line()+
  #expand_limits(y = c(0,1))+
  theme_bw()

```


```{r}
plot2<-aux %>%
  filter(
         max.depth %in% c(10,20,50,130)
         #,min.node.size %in% c(1,7,30)
         
         )
ggplot(plot2, aes(x=mtry, y = ROC,
                  #shape=as.factor(min.node.size),
                  color=as.factor(max.depth)
                  ))+
  geom_point(size=3,position=position_jitter(h=0, w=0))+
  geom_smooth(se=0,span = 0.2)+
  scale_x_continuous(breaks = (seq(10,130,20)))+
  #geom_line()+
  expand_limits(y = c(0.85,0.95))+
  theme_bw()
```

```{r}
plot2<-aux %>%
  filter(
         mtry %in% c(1,20,150),
         max.depth %in% c(10,50,90,130)
         )
ggplot(plot2, aes(x=min.node.size, y = ROC,
                  shape=as.factor(max.depth),
                  color=as.factor(mtry)))+
  geom_point(size=3,position=position_jitter(h=0, w=0.5))+
  geom_smooth(se=F)+
  #geom_line()+
  #expand_limits(y = c(0,1))+
  theme_bw()
```

