---
title: "Comparacion de Modelos"
output: html_document
---

# Paquetes

```{r}
library(caret)
library(readr)
library(dplyr)
library(ggplot2)
library(doMC) 
library(ggpubr)
```

# Dataset

```{r}
# Linux
data <- read_csv('Proyecto A/00 - Creación Dataset y Variables/Dataset_Modelos.csv')
```
```{r}
# Windows
data <- read_csv('../00 - Creación Dataset y Variables/Dataset_Modelos.csv')
```

Eliminamos las variables con caracteres para crear las particiones de train y test.

```{r}
char_vars <- c("X1", 
               "url", 
               "scheme", 
               "domain_complete", 
               "domain", 
               "subdomain", 
               "suffix", 
               "domain_subdomain", 
               "path", 
               "suffix2")

df <- data %>%
  select(-char_vars)
```

```{r}
set.seed(45)
train_index <- createDataPartition(y = df$phishing, times = 1, p = 0.80, list = FALSE)

train <- df[train_index,]
test <- df[-train_index,]

train <- train %>%
  mutate(phishing = factor(ifelse(phishing==1, "Phish", "NoPhish")))
test <- test %>%
  mutate(phishing = factor(ifelse(phishing==1, "Phish", "NoPhish")))

#train <- train[,1:44]
#test <- test[,1:44]

rm(df)
```



```{r}
getwd()
svm <- readRDS("C:/Users/jorge/OneDrive/Documentos/GitHub/Proyecto_CDD/Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/SVC.rds")
svmr <- readRDS("C:/Users/jorge/OneDrive/Documentos/GitHub/Proyecto_CDD/Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/SVCR.rds")
gbm <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/GBM.rds")
rf <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/RF.rds")
ridge <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/Ridge.rds")
lasso <- readRDS("Proyecto A/03 - Comparacion Modelos/Modelos Entrenados/Lasso.rds")
```

# Train

```{r}
mat_ridge1 <- confusionMatrix(data = as.factor(predict(ridge, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
print(mat_ridge1)
```


```{r}
mat_lasso1 <- confusionMatrix(data = as.factor(predict(lasso, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
print(mat_lasso1)
```

```{r}
mat_svm1 <- confusionMatrix(data = as.factor(predict(svm, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_svmr1 <- confusionMatrix(data = as.factor(predict(svmr, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_rf1 <- confusionMatrix(data = as.factor(predict(rf, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_gbm1 <- confusionMatrix(data = as.factor(predict(gbm, train)),
                reference = as.factor(train$phishing),
                positive = "Phish",
                mode = "everything")
```

# Test

```{r}
mat_ridge2 <- confusionMatrix(data = as.factor(predict(ridge, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_lasso2 <- confusionMatrix(data = as.factor(predict(lasso, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_svm2 <- confusionMatrix(data = as.factor(predict(svm, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_svmr2 <- confusionMatrix(data = as.factor(predict(svmr, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```


```{r}
mat_rf2 <- confusionMatrix(data = as.factor(predict(rf, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```

```{r}
mat_gbm2 <- confusionMatrix(data = as.factor(predict(gbm, test)),
                reference = as.factor(test$phishing),
                positive = "Phish",
                mode = "everything")
```



```{r}
twoClassSummary()
```



```{r}
#models <- c("Ridge", "Lasso", "RF", "GBM")
#results <- list(mat_ridge1, mat_lasso1, mat_rf1, mat_gbm1)
models <- c("SVM","SVM R")
results <- list(mat_svm1,mat_svmr1)
metrics <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall")

vec_models <- c()
vec_data <- c()
vec_metric <- c()
vec_value <- c()

i = 1
for(result in results){
  for(metric in metrics){
    vec_models <- append(vec_models, models[i])
    vec_data <- append(vec_data, "Train")
    vec_metric <- append(vec_metric, metric)
    if(metric == "Accuracy"){
      vec_value <- append(vec_value, result$overall[metric])  
    } else {
      vec_value <- append(vec_value, result$byClass[metric])  
    }
  }
  i = i + 1
}

train_results <- data.frame("Models" = vec_models,
                            "Data" = vec_data,
                            "Metric" = vec_metric,
                            "Value" = vec_value)

```

```{r}
#models <- c("Ridge", "Lasso", "RF", "GBM")
#results <- list(mat_ridge2, mat_lasso2, mat_rf2, mat_gbm2)
models <- c("SVM","SVM R")
results <- list(mat_svm2,mat_svmr2)
metrics <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall")

vec_models <- c()
vec_data <- c()
vec_metric <- c()
vec_value <- c()

i = 1
for(result in results){
  for(metric in metrics){
    vec_models <- append(vec_models, models[i])
    vec_data <- append(vec_data, "Test")
    vec_metric <- append(vec_metric, metric)
    if(metric == "Accuracy"){
      vec_value <- append(vec_value, result$overall[metric])  
    } else {
      vec_value <- append(vec_value, result$byClass[metric])  
    }
  }
  i = i + 1
}

test_results <- data.frame("Models" = vec_models,
                            "Data" = vec_data,
                            "Metric" = vec_metric,
                            "Value" = vec_value)

```

```{r}
train_results
test_results
```

```{r}
results_SVM <- rbind(train_results, test_results)
write_csv(results_SVM, file = "Resultados.csv", append=T)
```



```{r}
metrics <- c("Accuracy", "Sensitivity", "Specificity", "Precision", "Recall")
for (metric in metrics){
  print(results %>%
  filter(Metric == metric) %>%
  ggplot(aes(x = reorder(as.factor(Models), -Value), y = Value, color = Data)) +
  geom_point(size = 3) +
  expand_limits(y = c(0.80,1)) +
  labs(title = metric,
       y = metric,
       x = "Modelos") +
  theme_bw())
}
```


```{r}
results_proba <- data.frame("Phishing" = test$phishing,
                            "PredictProba" = predict(gbm, test, type = "prob"),
                            "URL" = data[-train_index,]$url)


results_proba
```


```{r, fig.width= 12, fig.height=6, warning=FALSE}
# Generamos el primer plot - Scatterplot
plot_prob1 <- ggplot(results_proba, aes(x = PredictProba.Phish, y = Phishing, color = Phishing))+
  geom_jitter(height = 0.2, width = 0, alpha = 0.3)+
  labs(title = "Scatterplot: Clase vs Probabilidad Estimada",
       #subtitle = "Mientras mayor sea la diferencia entre las clases ",
       x = "Probabilidad Estimada",
       y = "Clase")+
  theme_bw()

# Generamos el segundo plot - Histograma
plot_prob2 <- ggplot(results_proba, aes(PredictProba.Phish, fill = Phishing))+
  geom_histogram(position = "identity", alpha = 0.6, bins = 45)+
  labs(title = "Histograma: Probabilidades Predichas",
       #subtitle = "Si no hay intersección con un determinado threshold tendríamos discriminación perfecta",
       x = "Probabilidad Estimada",
       y = "")+
  theme_bw()

# Graficamos ambos plots en conjunto
ggarrange(plot_prob1, plot_prob2, ncol = 2, nrow = 1)
```








